{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework3.ipynb","provenance":[{"file_id":"1PhNPpklp9FbxJEtsZ8Jp9qXQa4aZDK5Y","timestamp":1575039678076}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"c9QcGnGPdX2C","colab_type":"text"},"source":["\n","**Install requirements**"]},{"cell_type":"code","metadata":{"id":"k9O3aM3Tb28q","colab_type":"code","outputId":"bb2a86e5-a142-479c-c195-df40c0616e44","executionInfo":{"status":"ok","timestamp":1578561554242,"user_tz":-60,"elapsed":16564,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}},"colab":{"base_uri":"https://localhost:8080/","height":200}},"source":["!pip3 install 'Pillow==6.1'\n","!pip3 install 'torch==1.3.1'\n","!pip3 install 'torchvision==0.4.2'\n","!pip3 install 'Pillow-SIMD'\n","!pip3 install 'tqdm'\n","#!pip3 install --upgrade 'pillow'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: Pillow==6.1 in /usr/local/lib/python3.6/dist-packages (6.1.0)\n","Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.17.5)\n","Requirement already satisfied: torchvision==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n","Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.17.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (6.1.0)\n","Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (6.0.0.post0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fo942LMOdlh4","colab_type":"text"},"source":["**Import libraries**"]},{"cell_type":"code","metadata":{"id":"DokFOdD1dJEl","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Subset, DataLoader\n","from torch.backends import cudnn\n","\n","from PIL import Image\n","import itertools\n","\n","import sys\n","import io\n","import copy \n","\n","try:\n","    from torch.hub import load_state_dict_from_url\n","except ImportError:\n","    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n","\n","from torch.autograd import Function\n","import torchvision\n","from torchvision import transforms\n","\n","from torchvision.models import alexnet\n","from torchvision.datasets import VisionDataset\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OIDLJuIXK_vh","colab_type":"text"},"source":["**Set Arguments**"]},{"cell_type":"code","metadata":{"id":"d5PkYfqfK_SA","colab_type":"code","colab":{}},"source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","\n","NUM_CLASSES = 7 \n","\n","BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n","                     # the batch size, learning rate should change by the same factor to have comparable results\n","\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using DSG\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n","NUM_EPOCH = 30       # Total number of training epochs (iterations over dataset)\n","GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n","LOG_FREQUENCY = 5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gwii0TBHvzh","colab_type":"text"},"source":["**Define Data Preprocessing**"]},{"cell_type":"code","metadata":{"id":"QUDdw4j2H0Mc","colab_type":"code","colab":{}},"source":["# Define transforms for training phase\n","transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n","                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n","                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n","                                                                   # Remember this when applying different transformations, otherwise you get an error\n","                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n","                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) \n","])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHu_YqL3amdN","colab_type":"text"},"source":["Dataset"]},{"cell_type":"code","metadata":{"id":"qhcy5J1RalpH","colab_type":"code","colab":{}},"source":["#Funzioni per il caricamento del dataset\n","\n","def pil_loader(path):\n","  # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","  img = Image.open(open(path, 'rb'), mode='r')\n","  return img.convert('RGB')\n","\n","def make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n","  images = []\n","  dir = os.path.expanduser(dir)\n","  if not ((extensions is None) ^ (is_valid_file is None)):\n","      raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n","  if extensions is not None:\n","      def is_valid_file(x):\n","          return has_file_allowed_extension(x, extensions)\n","  for target in sorted(class_to_idx.keys()):\n","      d = os.path.join(dir, target)\n","      if not os.path.isdir(d):\n","          continue\n","      for root, _, fnames in sorted(os.walk(d, followlinks=True)):\n","          for fname in sorted(fnames):\n","              path = os.path.join(root, fname)\n","              if is_valid_file(path):\n","                  item = (path, class_to_idx[target])\n","                  images.append(item)\n","\n","  return images\n","\n","def has_file_allowed_extension(filename, extensions):\n","  \"\"\"Checks if a file is an allowed extension.\n","  Args:\n","      filename (string): path to a file\n","      extensions (tuple of strings): extensions to consider (lowercase)\n","  Returns:\n","      bool: True if the filename ends with one of given extensions\n","  \"\"\"\n","  return filename.lower().endswith(extensions)\n","\n","def default_loader(path):\n","  from torchvision import get_image_backend\n","  if get_image_backend() == 'accimage':\n","      return accimage_loader(path)\n","  else:\n","      return pil_loader(path)\n","\n","\n","class PACS(VisionDataset):\n","\n","    def __init__(self, root, folds_to_use, split='train', transform=None, target_transform=None):\n","\n","        super(PACS, self).__init__(root, transform=transform, target_transform=target_transform)\n","\n","        IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n","\n","        aux_root = self.root + \"/\" + folds_to_use[0]\n","        classes, class_to_idx = self._find_classes(aux_root)\n","\n","        samples = []\n","        self.domains = []\n","\n","        for fold in folds_to_use:\n","          aux_root = self.root + \"/\" + fold\n","          aux_samples_tuple = make_dataset(aux_root, class_to_idx, IMG_EXTENSIONS)\n","          for s in aux_samples_tuple:\n","            self.domains.append(fold)\n","          samples += aux_samples_tuple\n","\n","         \n","        if len(samples) == 0:\n","            raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n","                                \"Supported extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n","            \n","           \n","        self.loader =  default_loader\n","        self.extensions = IMG_EXTENSIONS\n","\n","        self.classes = classes\n","        self.class_to_idx = class_to_idx\n","        self.samples = samples\n","        self.targets = [s[1] for s in samples]\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","\n","    def _find_classes(self, dir):\n","        if sys.version_info >= (3, 5):\n","            # Faster and available in Python 3.5 and above\n","            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n","        else:\n","            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n","        classes.sort()\n","\n","        class_to_idx = {classes[i]: i for i in range(len(classes))}\n","        return classes, class_to_idx\n","\n","    def __getitem__(self, index):\n","   # Provide a way to access image and label via index. Image should be a PIL Image  label can be int\n","      path, target = self.samples[index]\n","      sample = self.loader(path)\n","      if self.transform is not None:\n","          sample = self.transform(sample)\n","      if self.target_transform is not None:\n","          target = self.target_transform(target)\n","\n","      return sample, target\n","\n","    def __len__(self):\n","        length = len # Provide a way to get the length (number of elements) of the dataset\n","        return len(self.samples)\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BreUxyG3l24K","colab_type":"text"},"source":["DANN_AlexNet"]},{"cell_type":"code","metadata":{"id":"DiVDkkNVl564","colab_type":"code","colab":{}},"source":["__all__ = ['DANN_AlexNet', 'DANN_alexnet']\n","\n","\n","model_urls = {\n","    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n","}\n","\n","class ReverseLayerF(Function):\n","    # Forwards identity\n","    # Sends backward reversed gradients\n","    @staticmethod\n","    def forward(ctx, x, alpha):\n","        ctx.alpha = alpha\n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","      output = grad_output.neg() * ctx.alpha\n","      return output, None\n","\n","class DANN_AlexNet(nn.Module):\n","\n","  def __init__(self, num_classes=1000):\n","      super(DANN_AlexNet, self).__init__()\n","\n","      self.features = nn.Sequential(\n","          nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n","          nn.ReLU(inplace=True),\n","          nn.MaxPool2d(kernel_size=3, stride=2),\n","          nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","          nn.ReLU(inplace=True),\n","          nn.MaxPool2d(kernel_size=3, stride=2),\n","          nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","          nn.ReLU(inplace=True),\n","          nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","          nn.ReLU(inplace=True),\n","          nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","          nn.ReLU(inplace=True),\n","          nn.MaxPool2d(kernel_size=3, stride=2),\n","      )\n","      self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","\n","      self.classifier = nn.Sequential(\n","          nn.Dropout(),\n","          nn.Linear(256 * 6 * 6, 4096),\n","          nn.ReLU(inplace=True),\n","          nn.Dropout(),\n","          nn.Linear(4096, 4096),\n","          nn.ReLU(inplace=True),\n","          nn.Linear(4096, num_classes),\n","      )\n","      self.domain_classifier = nn.Sequential(\n","          nn.Dropout(),\n","          nn.Linear(256 * 6 * 6, 4096),\n","          nn.ReLU(inplace=True),\n","          nn.Dropout(),\n","          nn.Linear(4096, 4096),\n","          nn.ReLU(inplace=True),\n","          nn.Linear(4096, 2),\n","      )\n","\n","  def forward(self, x, alpha=None):\n","\n","      x = self.features(x)\n","      x = self.avgpool(x)\n","      x = torch.flatten(x, 1)\n","      if alpha is not None:\n","        reverse_x = ReverseLayerF.apply(x, alpha)\n","        return self.domain_classifier(reverse_x)\n","      else:\n","        return self.classifier(x)\n","\n","def DANN_alexnet(pretrained=False, progress=True, **kwargs):\n","\n","    model = DANN_AlexNet(**kwargs)\n","    if pretrained:\n","        Alex_state_dict = load_state_dict_from_url(model_urls['alexnet'],\n","                                              progress=progress)\n","        state_dict = model.state_dict() \n","\n","        for key, values in Alex_state_dict.items():\n","          state_dict[key] = values \n","          if key[:10] == \"classifier\":\n","            aux_str = \"classifier\" + key[10:]\n","            state_dict[aux_str] = values\n","\n","        model.load_state_dict(state_dict)\n","        \n","    model.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n","\n","    return model\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpfqM_NFdNEX","colab_type":"text"},"source":["Utility"]},{"cell_type":"code","metadata":{"id":"m9AVmaRNdMnG","colab_type":"code","colab":{}},"source":["'''Class used to print the details on the trend of the losses during the execution\n","for the point 4. The method print_log reports the various losses during the\n","phases of the execution while the method print_loss prints the losses in an\n","easily usable format to print graphics on Python.\n","\n","If you want to see the values of the loss remove comments from the last two\n","methods of the code that implements point 4A-B and point 4C-D.'''\n","\n","class Log:\n","\n","  def __init__(self, str):\n","    self.log = [[\"str\", str]]\n","\n","  def add_iper(self, list):\n","    self.log.append([\"iper\", list])\n","\n","  def add_epoch(self, list):\n","    self.log.append([\"epoch\", list])\n","\n","  def add_str(self, list):\n","    self.log.append([\"str\", list])\n","\n","  def add_loss(self, list):\n","    self.log.append([\"loss\", list])\n","\n","  def concLOG(self, other_Log):\n","    self.log.append([\"new log\", \"\"])\n","    self.add_str(\"\\n-------------------------------------------------------------\\n\")\n","    self.log += other_Log.log\n","\n","  def print_log(self):\n","\n","    for ele in self.log:\n","\n","      if ele [0] == \"iper\":\n","        \n","        if len(ele[1]) == 2:\n","          aux_str = \"Inizio stampa dati per: lr = \" + str(ele[1][0]) + \" step_size = \" +  str(ele[1][1])  + \".\\n\"\n","        else:\n","          aux_str = \"Inizio stampa dati per: lr = \" + str(ele[1][0]) + \" step_size = \" +  str(ele[1][1])  +  \" alpha = \" + str(ele[1][2]) + \".\\n\"\n","        print(aux_str)\n","\n","      elif ele[0] == \"epoch\":\n","        print (\"Starting epoch: \" + str(ele[1][0]) + \" / \" + str(str(ele[1][1])) + \" lr = \" + str(ele[1][2]) )\n","      \n","      elif ele[0] == \"loss\":\n","        aux_str = \"\"\n","        firts = True\n","\n","        for index in range(len(ele[1])):\n","          if firts:\n","            firts = False\n","            aux_str += \"  Loss per fase \" + str(index) + \": \" + str(ele[1][index])\n","          else:\n","            aux_str += \"\\n  Loss per fase \" + str(index) + \": \" + str(ele[1][index])\n","        \n","        print(aux_str)\n","      \n","      else:\n","        print(ele[1])\n","\n","  def print_loss(self):\n","\n","    str_aux = \"\\n\"\n","    loss1 = \"\"\n","    loss2 = \"\"\n","    first = True\n","    DANN = False\n","    last = False\n","    first_block = True\n","    first_ele = True\n","\n","    for ele in self.log:\n","\n","      if ele[0] == \"loss\":\n","        if first:\n","          str_aux += \"[\"\n","          first = False\n","          last = True\n","        \n","        elif len(ele[1]) == 1 :\n","          str_aux += \", \"\n","\n","        if len(ele[1]) == 1:\n","          str_aux += str(ele[1][0])\n","\n","        else:\n","          DANN = True\n","          if first_ele:\n","            loss1 += str(ele[1][0])\n","            loss2 += str(ele[1][1])\n","            first_ele = False\n","          else:\n","            loss1 += \", \" + str(ele[1][0])\n","            loss2 += \", \" + str(ele[1][1])\n","      \n","      elif ele[0] != \"epoch\":\n","       \n","        if DANN :\n","          str_aux += loss1 + \"], [\"\n","          str_aux += loss2 + \"]\"\n","          first_ele = True\n","          loss1 = \"\"\n","          loss2 = \"\"\n","          DANN = False\n","\n","        if last:\n","          str_aux += \"]\"\n","          first = True\n","          last = False\n","\n","        if ele[0] == \"new log\":\n","          if first_block:\n","            str_aux += \"[\"\n","            first_block = False\n","          else:\n","            str_aux += \"]\\n[\"\n","\n","\n","\n","\n","\n","    str_aux += \"]]\"\n","    print(str_aux)\n"," \n","def prepareDataloader(transform, source_domains, target_domains, val1_domains, val2_domains):\n","  # Clone github repository with data\n","  if not os.path.isdir('./Homework3-PACS'):\n","    !git clone https://github.com/MachineLearning2020/Homework3-PACS\n","\n","  DATA_DIR = 'Homework3-PACS/PACS'\n","\n","  # Prepare Pytorch Datasets\n","  domains = source_domains + target_domains + val1_domains + val2_domains\n","\n","\n","  dataset = PACS(DATA_DIR, domains, transform=transform)\n","\n","  source_indexes = []\n","  target_indexes = []\n","  val1_indexes = []\n","  val2_indexes = []\n","\n","\n","\n","  for index in range(len(dataset)):\n","    if dataset.domains[index] in source_domains: \n","      source_indexes.append(index)\n","    elif dataset.domains[index] in target_domains: \n","      target_indexes.append(index)\n","    elif dataset.domains[index] in val1_domains: \n","      val1_indexes.append(index)\n","    else:\n","      val2_indexes.append(index)\n","\n","  source_dataset = Subset(dataset, source_indexes)\n","  target_dataset = Subset(dataset, target_indexes)\n","  val1_dataset = Subset(dataset, val1_indexes)\n","  val2_dataset = Subset(dataset, val2_indexes)\n","\n","  # Check dataset sizes\n","  print('Art Painting Dataset: {}'.format(len(source_dataset)))\n","  print('Photo Dataset: {}'.format(len(target_dataset)))  \n","  print('Cartoon Dataset: {}'.format(len(val1_dataset)))  \n","  print('Sketch Dataset: {}'.format(len(val2_dataset)))  \n","\n","  # Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n","  source_dataloader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","  target_dataloader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","  val1_dataloader = DataLoader(val1_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","  val2_dataloader = DataLoader(val2_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","\n","\n","  return source_dataloader, target_dataloader, val1_dataloader, val2_dataloader\n","\n","def test (net, dataloader):\n","\n","  net.train(False)\n","  running_corrects = 0\n","\n","  for images, labels in dataloader:\n","\n","    images = images.to(DEVICE)\n","    labels = labels.to(DEVICE)\n","\n","    outputs = net(images)\n","    _, preds = torch.max(outputs.data, 1)\n","\n","    running_corrects += torch.sum(preds == labels.data).data.item()\n","    \n","  return running_corrects / float(len(dataloader.dataset))\n","\n","def test_accuracy_NO_DANN(source_dataloader, cartoon_dataloader, sketch_dataloader, lr, step_size):\n","  \n","  best_model = {'net': 0, 'acc':0, 'num_epoch': 0}\n","  net = DANN_alexnet(pretrained=True)\n","\n","  Log_aux = Log(\"Inizio stampa dati per: lr = \" + str(lr) + \" step_size = \" +  str(step_size) + \".\\n\")\n","\n","  # Define loss function\n","  criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n","\n","  #Ottimzzo solo i parametri coninvolti nella classificazione senza DANN\n","  params= [net.features.parameters(), net.classifier.parameters()]\n","\n","  parameters_to_not_optimize = net.domain_classifier.parameters()\n","  for p in parameters_to_not_optimize:\n","      p.requires_grad = False\n","\n","  # An optimizer updates the weights based on loss\n","  optimizer = optim.SGD(itertools.chain(*params), lr=lr, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","\n","  # A scheduler dynamically changes learning rate\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=GAMMA)\n","\n","  net = net.to(DEVICE)\n","  cudnn.benchmark # Calling this optimizes runtime\n","  accuracy_list = []\n","  \n","  # Start iterating over the epochs\n","\n","  for epoch in range(NUM_EPOCH):\n","\n","    Log_aux.add_epoch([(epoch+1), NUM_EPOCH, scheduler.get_lr()])\n","    aux_loss = 0\n","    num_batch = 0\n","\n","    # Iterate over the dataset\n","    for images, labels in source_dataloader:\n","\n","      # Bring data over the device of choice\n","      images = images.to(DEVICE)\n","      labels = labels.to(DEVICE)\n","\n","      net.train() # Sets module in training mode\n","\n","      # PyTorch, by default, accumulates gradients after each backward pass\n","      # We need to manually set the gradients to zero before starting a new iteration\n","      optimizer.zero_grad() # Zero-ing the gradients\n","\n","      # Forward pass to the network\n","      outputs = net(images)\n","\n","      # Compute loss based on output and ground truth\n","      loss = criterion(outputs, labels)\n","      if math.isnan(loss):\n","        Log_aux.add_loss([loss])\n","        return  best_model,  Log_aux \n","\n","      aux_loss += loss.item()\n","      num_batch += 1\n","\n","      # Compute gradients for each layer and update weights\n","      loss.backward()  # backward pass: computes gradients\n","      optimizer.step() # update weights based on accumulated gradients\n","    \n","    Log_aux.add_loss([aux_loss / num_batch])\n","\n","    # Step the scheduler\n","    scheduler.step() \n","    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","\n","    accuracy_sketch = test(net, sketch_dataloader)\n","    accuracy_cartoon = test(net, cartoon_dataloader)\n","\n","    accuracy_avg = (accuracy_cartoon + accuracy_sketch) / 2\n","    accuracy_avg = round(accuracy_avg * 100, 2)\n","\n","    if accuracy_avg > best_model['acc']:\n","      best_model['acc'] = accuracy_avg\n","      best_model['net'] = copy.deepcopy(net)\n","      best_model['num_epoch'] = epoch + 1\n","   \n","  \n","  return best_model, Log_aux \n","\n","def test_accuracy_DANN(source_dataloader, val_dataloader, lr, step_size, alpha, num_epoch):\n","\n","\n","  Log_aux = Log(\"Inizio stampa dati per: lr = \" + str(lr) + \", step_size = \" +  str(step_size) + \", alpha = \" + str(alpha) + \".\\n\")\n","\n","  net = DANN_alexnet(pretrained=True)\n","  # Define loss functions\n","  criterion_class = nn.CrossEntropyLoss() \n","  criterion_domain = nn.CrossEntropyLoss() \n","\n","  params = net.parameters()\n","  optimizer = optim.SGD(params, lr=lr, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=GAMMA)\n","\n","  net = net.to(DEVICE)\n","  cudnn.benchmark \n","  accuracy_list = []\n","\n","  for epoch in range(num_epoch):\n","\n","    Log_aux.add_epoch([epoch+1, num_epoch, scheduler.get_lr()])\n","\n","    net.train(True) \n","\n","    aux_val_dataloader = copy.deepcopy(val_dataloader)\n","    aux_val_iter_dataloader = iter(aux_val_dataloader)\n","\n","    aux_loss1 = 0\n","    aux_loss2 = 0\n","    num_batch = 0\n","\n","    # Iterate over the dataset\n","    for images_source, labels_source in source_dataloader:\n","\n","      images_val, labels_val = next(aux_val_iter_dataloader)\n","\n","      images_source = images_source.to(DEVICE)\n","      labels_source = labels_source.to(DEVICE)\n","      images_val = images_val.to(DEVICE)\n","\n","\n","      labels_source_domains = [0]*BATCH_SIZE\n","      labels_source_domains = torch.LongTensor(labels_source_domains)\n","      labels_source_domains = labels_source_domains.to(DEVICE)\n","\n","      labels_val_domains = [1]*BATCH_SIZE\n","      labels_val_domains = torch.LongTensor(labels_val_domains)\n","      labels_val_domains = labels_val_domains.to(DEVICE)\n","\n","\n","      net.train() # Sets module in training mode\n","      optimizer.zero_grad() # Zero-ing the gradients\n","\n","      ##################################################################\n","      #      train on source labels by forwarding source data to Gy    #\n","      ##################################################################\n","\n","      # Forward pass to the network\n","      outputs = net(images_source)\n","\n","      # Compute loss based on output and ground truth\n","      loss_classification = criterion_class(outputs, labels_source)\n","   \n","      # Calcola il gradiente e lo somma a quelli precedenti\n","      loss_classification.backward()\n","\n","      ###################################################################\n","      #     train the discriminator by forwarding source data to Gd     #\n","      ###################################################################\n","\n","      outputs = net(images_source, alpha)\n","\n","      loss_domain_source = criterion_domain(outputs, labels_source_domains)\n","      loss_domain_source.backward()  \n","\n","      ###################################################################\n","      #     train the discriminator by forwarding val data to Gd     #\n","      ###################################################################\n","\n","      outputs = net(images_val, alpha)\n","\n","      loss_domain_val = criterion_domain(outputs, labels_val_domains)\n","      loss_domain_val.backward()  \n","\n","\n","      optimizer.step() # update weights based on accumulated gradients\n","\n","      aux_loss1 += loss_classification.item()\n","      aux_loss2 += (loss_domain_source.item() + loss_domain_val.item())/2\n","      num_batch += 1\n","\n","    Log_aux.add_loss([aux_loss1 / num_batch, aux_loss2 / num_batch])\n","    # Step the scheduler\n","    scheduler.step() \n","\n","    # Calcolo accuracy sul val\n","    accuracy = test(net, val_dataloader)\n","    accuracy_list.append(round(accuracy*100, 2))\n","\n","    del aux_val_iter_dataloader\n","\n","  return accuracy_list, Log_aux\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2qYIHPzYLY7i","colab_type":"text"},"source":["**Prepare Dataloader**"]},{"cell_type":"code","metadata":{"id":"3AN6rRuGp6JY","colab_type":"code","outputId":"7ae78420-c53c-47f7-b53d-03bd4a10cb8b","executionInfo":{"status":"ok","timestamp":1578561555765,"user_tz":-60,"elapsed":18001,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}},"colab":{"base_uri":"https://localhost:8080/","height":98}},"source":["%xmode Verbose\n","source_domains = [\"photo\"]\n","target_domains = [\"art_painting\"]\n","cartoon_domains = [\"cartoon\"]\n","sketch_domains = [\"sketch\"]\n","\n","source_dataloader, target_dataloader, cartoon_dataloader, sketch_dataloader = prepareDataloader(transform, source_domains, target_domains, cartoon_domains, sketch_domains)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n","Art Painting Dataset: 1670\n","Photo Dataset: 2048\n","Cartoon Dataset: 2344\n","Sketch Dataset: 3929\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JyXag_n-0N86","colab_type":"text"},"source":["3A"]},{"cell_type":"code","metadata":{"id":"J--eRhma3bsO","colab_type":"code","colab":{}},"source":["#Hyperparameters for point 3A and 3B\n","LR = 1e-4\n","STEP_SIZE = 25\n","ALPHA = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfVq_uDHLbsR","colab_type":"code","outputId":"8c1c3b3b-19cd-49f1-8281-99e79eb9a672","executionInfo":{"status":"ok","timestamp":1578561567116,"user_tz":-60,"elapsed":29334,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}},"colab":{"base_uri":"https://localhost:8080/","height":98}},"source":["\n","%xmode Verbose\n","net = DANN_alexnet(pretrained=True)\n","\n","# Define loss function\n","criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n","\n","#Ottimzzo solo i parametri coninvolti nella classificazione senza DANN\n","params= [net.features.parameters(), net.classifier.parameters()]\n","\n","parameters_to_not_optimize = net.domain_classifier.parameters()\n","for p in parameters_to_not_optimize:\n","    p.requires_grad = False\n","\n","# An optimizer updates the weights based on loss\n","optimizer = optim.SGD(itertools.chain(*params), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","\n","# A scheduler dynamically changes learning rate\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","net = net.to(DEVICE)\n","\n","cudnn.benchmark # Calling this optimizes runtime\n","\n","current_step = 0\n","# Start iterating over the epochs\n","for epoch in range(NUM_EPOCH):\n","  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCH, scheduler.get_lr()))\n","\n","  # Iterate over the dataset\n","  for images, labels in source_dataloader:\n","\n","    # Bring data over the device of choice\n","    images = images.to(DEVICE)\n","    labels = labels.to(DEVICE)\n","\n","    net.train() # Sets module in training mode\n","\n","    # PyTorch, by default, accumulates gradients after each backward pass\n","    # We need to manually set the gradients to zero before starting a new iteration\n","    optimizer.zero_grad() # Zero-ing the gradients\n","\n","    # Forward pass to the network\n","    outputs = net(images)\n","\n","    # Compute loss based on output and ground truth\n","    loss = criterion(outputs, labels)\n","\n","    # Log loss\n","    if current_step % LOG_FREQUENCY == 0:\n","      print('   Step {}, Loss {}'.format(current_step, loss.item()))\n","\n","    # Compute gradients for each layer and update weights\n","    loss.backward()  # backward pass: computes gradients\n","    optimizer.step() # update weights based on accumulated gradients\n","\n","    current_step += 1\n","\n","  # Step the scheduler\n","  scheduler.step() \n","  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","\n","net.train(False) # Set Network to evaluation mode\n","\n","running_corrects = 0\n","\n","for images, labels in target_dataloader:\n","  images = images.to(DEVICE)\n","  labels = labels.to(DEVICE)\n","\n","  # Forward Pass\n","  outputs = net(images)\n","\n","  # Get predictions\n","  _, preds = torch.max(outputs.data, 1)\n","\n","\n","  # Update Corrects\n","  running_corrects += torch.sum(preds == labels.data).data.item()\n","  \n","\n","# Calculate Accuracy\n","accuracy = running_corrects / float(len(target_dataloader.dataset))\n","\n","print('Test Accuracy: {}%.'.format(round(accuracy*100, 2)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n","Starting epoch 1/1, LR = [0.0001]\n","   Step 0, Loss 2.1150975227355957\n","   Step 5, Loss 1.8422948122024536\n","Test Accuracy: 16.6%.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M-Uix2t-OSRQ","colab_type":"text"},"source":["3B"]},{"cell_type":"code","metadata":{"id":"BjuFY8iDOT9m","colab_type":"code","outputId":"8f060488-8404-48d4-cdab-185b71b5cdc6","executionInfo":{"status":"ok","timestamp":1578561577556,"user_tz":-60,"elapsed":39765,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["\n","%xmode Verbose\n","net = DANN_alexnet(pretrained=True)\n","\n","# Define loss function\n","criterion_class = nn.CrossEntropyLoss() \n","criterion_domain = nn.CrossEntropyLoss() \n","\n","\n","#Ottimzzo tutti i parametri della rete\n","params= net.parameters()\n","\n","# An optimizer updates the weights based on loss\n","optimizer = optim.SGD(params, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","\n","# A scheduler dynamically changes learning rate\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","net = net.to(DEVICE)\n","cudnn.benchmark # Calling this optimizes runtime\n","current_step = 0\n","\n","for epoch in range(NUM_EPOCH):\n","\n","  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCH, scheduler.get_lr()))\n","\n","  aux_target_dataloader = copy.deepcopy(target_dataloader)\n","  aux_target_iter_dataloader = iter(aux_target_dataloader)\n","\n","  # Iterate over the dataset\n","  for images_source, labels_source in source_dataloader:\n","\n","    images_target, labels_target = next(aux_target_iter_dataloader)\n","\n","    images_source = images_source.to(DEVICE)\n","    labels_source = labels_source.to(DEVICE)\n","    images_target = images_target.to(DEVICE)\n","    labels_target = labels_target.to(DEVICE)\n","\n","\n","    net.train() # Sets module in training mode\n","    optimizer.zero_grad() # Zero-ing the gradients\n","\n","    #################\n","    #     3.B.1     #\n","    #################\n","\n","    # Forward pass to the network\n","    outputs = net(images_source)\n","\n","    # Compute loss based on output and ground truth\n","    loss_classification = criterion_class(outputs, labels_source)\n","\n","  \n","    # Compute gradients for each layer and update weights\n","    loss_classification.backward()  # backward pass: computes gradients\n","\n","    #################\n","    #     3.B.2     #\n","    #################\n","\n","    outputs = net(images_source, ALPHA)\n","\n","    labels_source = [0]*BATCH_SIZE\n","    labels_source = torch.LongTensor(labels_source)\n","    labels_source = labels_source.to(DEVICE)\n","\n","\n","    loss_photo = criterion_domain(outputs, labels_source)\n","    loss_photo.backward()  \n","\n","\n","    #################\n","    #     3.B.3     #\n","    #################\n","\n","    outputs = net(images_target, ALPHA)\n","\n","    labels_target = [1]*BATCH_SIZE\n","    labels_target = torch.LongTensor(labels_target)\n","    labels_target = labels_target.to(DEVICE)\n","\n","    loss_art = criterion_domain(outputs, labels_target)\n","    loss_art.backward()  \n","\n","\n","    optimizer.step() # update weights based on accumulated gradients\n","\n","  # Log loss\n","    if current_step % LOG_FREQUENCY == 0:\n","      print(\"   Step {\", current_step, \"}, Loss {\", loss_classification.item(), \"} on 3.B.1.\")\n","      print(\"   Step {\", current_step, \"}, Loss {\", loss_photo.item(), \"} on 3.B.2.\")\n","      print(\"   Step {\", current_step, \"}, Loss {\", loss_art.item(), \"} on 3.B.3.\")\n","    current_step += 1\n","\n","  # Step the scheduler\n","  scheduler.step() \n","  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","\n","\n","\n","net.train(False) # Set Network to evaluation mode\n","running_corrects = 0\n","\n","for images, labels in target_dataloader:\n","\n","  images = images.to(DEVICE)\n","  labels = labels.to(DEVICE)\n","\n","  # Forward Pass\n","  outputs = net(images)\n","\n","  # Get predictions\n","  _, preds = torch.max(outputs.data, 1)\n","\n","\n","  # Update Corrects\n","  running_corrects += torch.sum(preds == labels.data).data.item()\n","  \n","\n","# Calculate Accuracy\n","accuracy = running_corrects / float(len(target_dataloader.dataset))\n","\n","print('Test Accuracy: {}%.'.format(round(accuracy*100, 2)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n","Starting epoch 1/1, LR = [0.0001]\n","   Step { 0 }, Loss { 2.3229129314422607 } on 3.B.1.\n","   Step { 0 }, Loss { 0.7061945796012878 } on 3.B.2.\n","   Step { 0 }, Loss { 0.7256376147270203 } on 3.B.3.\n","   Step { 5 }, Loss { 1.9416784048080444 } on 3.B.1.\n","   Step { 5 }, Loss { 0.7526364922523499 } on 3.B.2.\n","   Step { 5 }, Loss { 0.6706061959266663 } on 3.B.3.\n","Test Accuracy: 15.48%.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dG4QCRj83Ko0","colab_type":"text"},"source":["**4.A e 4.B**"]},{"cell_type":"code","metadata":{"id":"ws2Dcr9ji4xY","colab_type":"code","colab":{}},"source":["#Hyperparameter for point 4A-B\n","STEP_SIZES = [25, 30]    \n","LRS= [1e-2, 5e-3, 1e-3] "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"keJxBB273JtP","colab_type":"code","outputId":"fa2e09cd-7caa-4f94-e1bd-944fb34e5b70","executionInfo":{"status":"ok","timestamp":1578561658436,"user_tz":-60,"elapsed":120613,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}},"colab":{"base_uri":"https://localhost:8080/","height":505}},"source":["%xmode Verbose\n","\n","top_model = { 'lr':0, 'acc': 0, 'step_size': 0, 'num_epoch': 0}\n","Log_1 = Log(\"\")\n","\n","for lr in LRS:\n","  for step_size in STEP_SIZES:\n","\n","    print(\"------------------------------------------------------------------------\")\n","    print(\"Starting ( LR =\", lr,  \", Step size =\", step_size, \").\")\n","\n","    best_model_test, Log_aux = test_accuracy_NO_DANN(source_dataloader, sketch_dataloader, cartoon_dataloader, lr, step_size)   \n","    Log_1.concLOG(Log_aux)    \n","\n","    print(\"Ending ( LR =\", lr,  \", Step size =\", step_size, \"Num epoch =\", best_model_test['num_epoch'], \").\")\n","    print(\"Accuratezza media migliore \", best_model_test['acc'], \".\")\n","\n","    if top_model['acc'] < best_model_test['acc'] :\n","      top_model['lr'] = lr\n","      top_model['acc'] = best_model_test['acc']\n","      top_model['step_size'] = step_size\n","      top_model['num_epoch'] = best_model_test['num_epoch']\n","      top_model['net'] = best_model_test['net']\n","\n","print(\"------------------------------------------------------------------------\")\n","print(\"Iperparametri migliori: lr=\", top_model['lr'], \", step_size=\", top_model['step_size'], \". Accurattezza massima =\", top_model['acc'], \". Numero di epoche:\", top_model['num_epoch'])\n","\n","print(\"\\n\\n-------------------> Accuracy sul target domain:\", str(round(test(top_model['net'], target_dataloader)*100, 2)) + \"%.\")\n","\n","#Log_1.print_log()\n","#Log_1.print_loss()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n","------------------------------------------------------------------------\n","Starting ( LR = 0.01 , Step size = 25 ).\n","Ending ( LR = 0.01 , Step size = 25 Num epoch = 1 ).\n","Accuratezza media migliore  19.75 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.01 , Step size = 30 ).\n","Ending ( LR = 0.01 , Step size = 30 Num epoch = 1 ).\n","Accuratezza media migliore  22.47 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.005 , Step size = 25 ).\n","Ending ( LR = 0.005 , Step size = 25 Num epoch = 1 ).\n","Accuratezza media migliore  27.3 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.005 , Step size = 30 ).\n","Ending ( LR = 0.005 , Step size = 30 Num epoch = 1 ).\n","Accuratezza media migliore  23.47 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.001 , Step size = 25 ).\n","Ending ( LR = 0.001 , Step size = 25 Num epoch = 1 ).\n","Accuratezza media migliore  29.84 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.001 , Step size = 30 ).\n","Ending ( LR = 0.001 , Step size = 30 Num epoch = 1 ).\n","Accuratezza media migliore  27.97 .\n","------------------------------------------------------------------------\n","Iperparametri migliori: lr= 0.001 , step_size= 25 . Accurattezza massima = 29.84 . Numero di epoche: 1\n","\n","\n","-------------------> Accuracy sul target domain: 44.19%.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jTGDHQtRndSV","colab_type":"code","colab":{}},"source":["#Hyperparameters for point 4C-D\n","LRS= [1e-3, 5e-4, 1e-4]           \n","STEP_SIZES = [25, 30]       \n","ALPHAS = [0.5, 0.3, 0.1]        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I2bvf015HXua","colab_type":"text"},"source":["**4.C e 4.D**"]},{"cell_type":"code","metadata":{"id":"lN4O2AeXHayw","colab_type":"code","outputId":"b77a2212-e20d-4fc8-8b96-d4fcd02d55d3","executionInfo":{"status":"ok","timestamp":1578588691291,"user_tz":-60,"elapsed":133272,"user":{"displayName":"Giulio Bagnoli","photoUrl":"","userId":"16972768245043587235"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%xmode Verbose\n","\n","top_model = { 'lr':0, 'acc': 0, 'step_size': 0, 'num_epoch': 0, 'alpha': 0}\n","Log_1 = Log(\"\")\n","\n","for lr in LRS:\n","  for step_size in STEP_SIZES:\n","    for alpha in ALPHAS:\n","\n","      print(\"------------------------------------------------------------------------\")\n","      print(\"Starting ( LR =\", lr,  \", Step size =\", step_size, \", Alpha = \", alpha, \").\")\n","\n","      accuracy_list_sketch, log_aux1 = test_accuracy_DANN(source_dataloader, sketch_dataloader, lr, step_size, alpha, NUM_EPOCH)      \n","      accuracy_list_cartoon, log_aux2 = test_accuracy_DANN(source_dataloader, cartoon_dataloader, lr, step_size, alpha, NUM_EPOCH)\n","\n","      Log_1.concLOG(log_aux1)\n","      Log_1.concLOG(log_aux2)\n","\n","      accuracy_list_avg = []\n","      for index in range(len(accuracy_list_sketch)):\n","        accuracy_list_avg.append((accuracy_list_sketch[index] + accuracy_list_cartoon[index])/2)\n","\n","      accuracy_avg_max = max(accuracy_list_avg)\n","      num_epoch = accuracy_list_avg.index(accuracy_avg_max) + 1\n","\n","      print(\"Ending ( LR =\", lr,  \", Step size =\", step_size,\"Alpha =\", alpha,  \"num epoch =\", num_epoch, \").\")\n","      print(\"Accuratezza media massima:\", accuracy_avg_max, \".\")\n","\n","      if top_model['acc'] < accuracy_avg_max :\n","        top_model['lr'] = lr\n","        top_model['acc'] = accuracy_avg_max\n","        top_model['step_size'] = step_size\n","        top_model['num_epoch'] = num_epoch\n","        top_model['alpha'] = alpha\n","\n","print(\"------------------------------------------------------------------------\")\n","print(\"Iperparametri migliori: lr=\", top_model['lr'], \", step_size=\", top_model['step_size'], \". Accurattezza massima =\", top_model['acc'], \". Numero di epoche:\", top_model['num_epoch'])\n","\n","accuracy_target = test_accuracy_DANN(source_dataloader, target_dataloader, top_model['lr'], top_model['step_size'],top_model['alpha'], top_model['num_epoch'])\n","print(\" --------------------------------> Accuracy sul target domain:\", accuracy_target[0][len(accuracy_target[0]) - 1])\n","\n","#Log_1.print_log()\n","Log_1.print_loss()\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n","------------------------------------------------------------------------\n","Starting ( LR = 0.001 , Step size = 25 , Alpha =  0.5 ).\n","Ending ( LR = 0.001 , Step size = 25 Alpha = 0.5 num epoch = 21 ).\n","Accuratezza media massima: 37.99 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.001 , Step size = 25 , Alpha =  0.3 ).\n","Ending ( LR = 0.001 , Step size = 25 Alpha = 0.3 num epoch = 26 ).\n","Accuratezza media massima: 33.595 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.001 , Step size = 25 , Alpha =  0.1 ).\n","Ending ( LR = 0.001 , Step size = 25 Alpha = 0.1 num epoch = 18 ).\n","Accuratezza media massima: 28.634999999999998 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.001 , Step size = 30 , Alpha =  0.5 ).\n","Ending ( LR = 0.001 , Step size = 30 Alpha = 0.5 num epoch = 18 ).\n","Accuratezza media massima: 37.405 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.001 , Step size = 30 , Alpha =  0.3 ).\n","Ending ( LR = 0.001 , Step size = 30 Alpha = 0.3 num epoch = 30 ).\n","Accuratezza media massima: 37.455 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.001 , Step size = 30 , Alpha =  0.1 ).\n","Ending ( LR = 0.001 , Step size = 30 Alpha = 0.1 num epoch = 1 ).\n","Accuratezza media massima: 26.41 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0005 , Step size = 25 , Alpha =  0.5 ).\n","Ending ( LR = 0.0005 , Step size = 25 Alpha = 0.5 num epoch = 26 ).\n","Accuratezza media massima: 37.45 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0005 , Step size = 25 , Alpha =  0.3 ).\n","Ending ( LR = 0.0005 , Step size = 25 Alpha = 0.3 num epoch = 2 ).\n","Accuratezza media massima: 27.655 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0005 , Step size = 25 , Alpha =  0.1 ).\n","Ending ( LR = 0.0005 , Step size = 25 Alpha = 0.1 num epoch = 24 ).\n","Accuratezza media massima: 24.509999999999998 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0005 , Step size = 30 , Alpha =  0.5 ).\n","Ending ( LR = 0.0005 , Step size = 30 Alpha = 0.5 num epoch = 30 ).\n","Accuratezza media massima: 38.145 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0005 , Step size = 30 , Alpha =  0.3 ).\n","Ending ( LR = 0.0005 , Step size = 30 Alpha = 0.3 num epoch = 30 ).\n","Accuratezza media massima: 28.67 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0005 , Step size = 30 , Alpha =  0.1 ).\n","Ending ( LR = 0.0005 , Step size = 30 Alpha = 0.1 num epoch = 2 ).\n","Accuratezza media massima: 29.490000000000002 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0001 , Step size = 25 , Alpha =  0.5 ).\n","Ending ( LR = 0.0001 , Step size = 25 Alpha = 0.5 num epoch = 4 ).\n","Accuratezza media massima: 24.0 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0001 , Step size = 25 , Alpha =  0.3 ).\n","Ending ( LR = 0.0001 , Step size = 25 Alpha = 0.3 num epoch = 26 ).\n","Accuratezza media massima: 20.63 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0001 , Step size = 25 , Alpha =  0.1 ).\n","Ending ( LR = 0.0001 , Step size = 25 Alpha = 0.1 num epoch = 3 ).\n","Accuratezza media massima: 21.869999999999997 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0001 , Step size = 30 , Alpha =  0.5 ).\n","Ending ( LR = 0.0001 , Step size = 30 Alpha = 0.5 num epoch = 8 ).\n","Accuratezza media massima: 28.255 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0001 , Step size = 30 , Alpha =  0.3 ).\n","Ending ( LR = 0.0001 , Step size = 30 Alpha = 0.3 num epoch = 30 ).\n","Accuratezza media massima: 21.835 .\n","------------------------------------------------------------------------\n","Starting ( LR = 0.0001 , Step size = 30 , Alpha =  0.1 ).\n","Ending ( LR = 0.0001 , Step size = 30 Alpha = 0.1 num epoch = 10 ).\n","Accuratezza media massima: 28.0 .\n","------------------------------------------------------------------------\n","Iperparametri migliori: lr= 0.0005 , step_size= 30 . Accurattezza massima = 38.145 . Numero di epoche: 30\n"," --------------------------------> Accuracy sul target domain: 48.49\n","\n","[[1.463958889245987, 0.4677407691876094, 0.2183072566986084, 0.15338301037748656, 0.12867754946152368, 0.10290560498833656, 0.10322569559017818, 0.09437650442123413, 0.0902188556889693, 0.10813118517398834, 0.11143711830178897, 0.09360998744765918, 0.08697348088026047, 0.06960822952290376, 0.06494925916194916, 0.054723174000779785, 0.0435477290302515, 0.03784699738025665, 0.03441997400174538, 0.036221411700050034, 0.030886230369408924, 0.02993594016879797, 0.021222439128905535, 0.02162085163096587, 0.024276069986323517, 0.02342458876470725, 0.019675442793716986, 0.019305468226472538, 0.02386559049288432, 0.02191657138367494], [0.5195376003781954, 0.1233566307152311, 0.04204015426027278, 0.032964992336928844, 0.04311627196148038, 0.06668685392166178, 0.09526966015497844, 0.13914473013331494, 0.18830812396481633, 0.23744520917534828, 0.28487462426225346, 0.292445523198694, 0.2774226694988708, 0.26011170912534, 0.24554032715968788, 0.23337494566415748, 0.22187801264226437, 0.20988955727079883, 0.19937116319003204, 0.18716677346189195, 0.17711379996035248, 0.1676945637445897, 0.15797718690009788, 0.1495131329090024, 0.14135425056641301, 0.1371098142117262, 0.13584261515643448, 0.1357288425012181, 0.13436289546856037, 0.13330078528573117]]]\n","[[1.5344954033692677, 0.47143668433030445, 0.22567953169345856, 0.15805120021104813, 0.13148337602615356, 0.11455563952525456, 0.10463646178444226, 0.08408681489527225, 0.07593120510379474, 0.0738546506812175, 0.07020995765924454, 0.07794452582796414, 0.075852960969011, 0.07121916860342026, 0.0741663792481025, 0.08763489685952663, 0.0737872173388799, 0.07342819993694623, 0.08265470465024312, 0.11285059029857318, 0.14544280618429184, 0.16143278032541275, 0.2117755189538002, 0.47957517206668854, 162.2444091041883, 3.3618874549865723, 3.4701994260152182, 3.05166224638621, 2.6319796244303384, 2.344062169392904], [0.6358158538738886, 0.3213827821115653, 0.17369627145429453, 0.10242892398188512, 0.08997063804417849, 0.07764661482845743, 0.07885998417623341, 0.07201530085876584, 0.08228940695213775, 0.07715006576230128, 0.08351440479358037, 0.09292463601256411, 0.10920450751048823, 0.11932300139839451, 0.12886555586010218, 0.12285991106182337, 0.13056321255862713, 0.1696880323191484, 0.22269809370239577, 0.29068640743692714, 0.35648801426092785, 0.5113027932432791, 0.6737061737415692, 2.366768665611744, 64.79254821047653, 5.421226687574138, 2.8169374405406415, 2.6266877508411803, 1.5421502102787297, 1.0758710720886786]]]\n","[[1.381926993529002, 0.441431259115537, 0.19985361148913702, 0.14644653225938478, 0.121375838915507, 0.09214555347959201, 0.07806109761198361, 0.06955535647769769, 0.05613930088778337, 0.045715637505054474, 0.03963406632343928, 0.03798718564212322, 0.03982822162409624, 0.028148936107754707, 0.026337535120546818, 0.0259380666539073, 0.0232023602972428, 0.023852363384018343, 0.023513955374558766, 0.01977887749671936, 0.018033705341319244, 0.01601384688789646, 0.014763903028021256, 0.015946289524435997, 0.016470459134628374, 0.013708166312426329, 0.013926193738977114, 0.015123872396846613, 0.014045667524139086, 0.01686550521602233], [0.4996294093628724, 0.11235020247598489, 0.025927106539408367, 0.010847112668367723, 0.007822975089463094, 0.007110533265707393, 0.006025737345529099, 0.005246401046557973, 0.005013833094077806, 0.005444900055105488, 0.00490638588477547, 0.004050194790276389, 0.004548449282689641, 0.004071249160915613, 0.004179982798329244, 0.004285723145585507, 0.0043799026946847635, 0.004054427845403552, 0.003915057692211121, 0.003829988573367397, 0.00481817836407572, 0.00479352295709153, 0.00368777794453005, 0.004020343301817775, 0.0037550281267613173, 0.0035313716895567873, 0.004257657196527968, 0.00400068275242423, 0.004073384605968992, 0.0035920366450833776]]]\n","[[1.5462801456451416, 0.5231006046136221, 0.22173218180735907, 0.15460470567146936, 0.12451870366930962, 0.09709120914340019, 0.0837265911201636, 0.07784672205646832, 0.071401530255874, 0.06703591098388036, 0.0490019346276919, 0.044338938469688095, 0.042010082552830376, 0.03834740196665128, 0.03641561511904001, 0.03814662309984366, 0.032917855928341545, 0.034734622575342655, 0.029252334497869015, 0.027732588971654575, 0.028243521228432655, 0.023885289207100868, 0.02150801041473945, 0.021359655385216076, 0.022533615119755268, 0.019800687208771706, 0.022191239365686972, 0.019886035782595474, 0.021734670270234346, 0.017579137813299894], [0.619841476281484, 0.31863026445110637, 0.15863661002367735, 0.09373130804548661, 0.08042655978351831, 0.061330767814069986, 0.06255166651681066, 0.05718787918643405, 0.05598569381982088, 0.055466404262309275, 0.052030046858514346, 0.05092767571719984, 0.055553472406851746, 0.04971114476211369, 0.05391364323440939, 0.05498821598788103, 0.04982728965114802, 0.05521517937692503, 0.05041469742233554, 0.052967327064834535, 0.04866482084617019, 0.05443476990330964, 0.048704823711887, 0.06503655939983825, 0.06067136519898971, 0.05631001084111631, 0.053926526413609586, 0.052264454774558544, 0.05269527497390906, 0.051382653104762234]]]\n","[[1.4190955261389415, 0.4465969304243724, 0.20174739758173624, 0.14260208482543626, 0.11460076769193013, 0.08924490461746852, 0.07137194089591503, 0.06305721153815587, 0.04900161021699508, 0.044052752976616226, 0.04169711625824372, 0.03927376742164294, 0.03050273967285951, 0.029517661159237225, 0.02950223721563816, 0.02878427505493164, 0.02595142259572943, 0.021553094343592722, 0.021891947835683823, 0.01823438486705224, 0.017026021455725033, 0.015573386879016956, 0.014064982378234466, 0.012099555072685083, 0.011826926531891028, 0.01071106611440579, 0.011818953168888887, 0.010785192095985016, 0.012750353043278059, 0.01229857342938582], [0.4926631848017375, 0.10969729938854773, 0.019471673023266096, 0.006230853924838205, 0.0034544781471292176, 0.0023917974807166806, 0.002155992241265873, 0.0018521887444270153, 0.0013867417292203754, 0.0019126840828297038, 0.0011877270299009979, 0.0014282990402231615, 0.0015029286635884394, 0.0012665535711372893, 0.0014280791898878913, 0.0009727793415853133, 0.001339514972642064, 0.0011545877787284553, 0.0010439628191913168, 0.0012400823664696266, 0.0005305121788599839, 0.0005977847031317651, 0.0007021489048687121, 0.0004777913078820954, 0.00048778972510869306, 0.0005700244995144507, 0.0006574075863075753, 0.0006715036967458824, 0.0005435168859548867, 0.0005191544769331813]]]\n","[[1.5113792916138966, 0.46211286385854083, 0.20152595390876135, 0.1364210546016693, 0.11848168323437373, 0.086361409475406, 0.07467184029519558, 0.07140361331403255, 0.0548193504412969, 0.055777465303738914, 0.047243516271313034, 0.04084873882432779, 0.03912414672474066, 0.03321319539099932, 0.029658834139506023, 0.023384814771513145, 0.024018132748703163, 0.020729731768369675, 0.022595585168649752, 0.018524354478965204, 0.017876031808555126, 0.016722325701266527, 0.01828918109337489, 0.013180414990832409, 0.013843621282527844, 0.013641410352041325, 0.012475392160316309, 0.012336130253970623, 0.013340371350447336, 0.011179802939295769], [0.6331501553455988, 0.31207841262221336, 0.1460923838118712, 0.08140834700316191, 0.0668510568793863, 0.04806362969490389, 0.04769783890030036, 0.0427210508302475, 0.04028412575523058, 0.03554569592233747, 0.02905643030923481, 0.03550596508042266, 0.027675486480196316, 0.032428035008100174, 0.02348160578791673, 0.02260334122305115, 0.02428877733958264, 0.026177755595805745, 0.021608801481003564, 0.02326408925000578, 0.01838125366096695, 0.017745320166189533, 0.02044523326912895, 0.017808786244131625, 0.01783843874000013, 0.019939976763756324, 0.01659001442991818, 0.01672440809973826, 0.014577607352597019, 0.01550165592925623]]]\n","[[1.4457499583562214, 0.45209112266699475, 0.21297178665796915, 0.14782049506902695, 0.11635903020699818, 0.10658096025387447, 0.09696869738399982, 0.10668241729338963, 0.08919966965913773, 0.09629827613631885, 0.10546908527612686, 0.11236286411682765, 0.1028921144704024, 0.08823308969537418, 0.0734783336520195, 0.05328626185655594, 0.045529606441656746, 0.04248919431120157, 0.036023570224642754, 0.032604655250906944, 0.032012664868185915, 0.028938847593963146, 0.02932971032957236, 0.02611486582706372, 0.021974538452923298, 0.019066308935483296, 0.01900550350546837, 0.020121068383256595, 0.019878016629566748, 0.01621023239567876], [0.4909997122983138, 0.12186072704692681, 0.03862419414023558, 0.028762785096963246, 0.034754770497481026, 0.05120796337723732, 0.07692523580044508, 0.11281852656975389, 0.15969774049396315, 0.20804553820441166, 0.2555046919733286, 0.27627973615502316, 0.2819265470219155, 0.268386385946845, 0.2487922420259565, 0.23677480407059193, 0.2270956695623075, 0.21588290036500743, 0.2043743822723627, 0.19428832471991578, 0.1837558803963475, 0.17386058915872127, 0.16353283527617654, 0.15476729850828028, 0.1453441435296554, 0.137421926871563, 0.1302833761825847, 0.12317864610425507, 0.11733582790475339, 0.1107973118002216]]]\n","[[1.4560048182805378, 0.4607429752747218, 0.2134865274031957, 0.15128537515799204, 0.11697549124558766, 0.09004707013567288, 0.09301162511110306, 0.08081990045805772, 0.07731967667738597, 0.06724584723512332, 0.07175781950354576, 0.07950407266616821, 0.10116923227906227, 0.08536140869061153, 0.08071241031090419, 0.07124911993741989, 0.07933553432424863, 0.0831955981751283, 0.09149513145287831, 0.13313662757476172, 0.21054609368244806, 0.2843798299630483, 40.00104337930679, 9.94075208902359, 1.8708666165669758, 3.6937632163365683, 9.387980540593466, 1.8776918450991313, 1.469649871190389, 1.3082166115442913], [0.6162205512324969, 0.2913077585399151, 0.14901997335255146, 0.10238694818690419, 0.08269492288430531, 0.0814471725995342, 0.07839741697534919, 0.07996809979279836, 0.08636187094574173, 0.09531928435899317, 0.1304006123294433, 0.15300228648508588, 0.14072113453100124, 0.11193073587492108, 0.11425732014079888, 0.14268675850083432, 0.16210585739463568, 0.19543475843966007, 0.2857104775806268, 0.4432647650440534, 0.46565805344531935, 0.8924798294901848, 23.043638098674517, 3.6079565277323127, 1.3782946344775457, 2.7207271941782287, 3.0059669706970453, 0.7552793584764004, 0.4075734466314316, 0.47160199905435246]]]\n","[[1.4660040736198425, 0.48314358790715534, 0.20924537380536398, 0.14640792707602182, 0.12417252237598102, 0.0995801215370496, 0.08942442884047826, 0.06979443008701007, 0.06407295477886994, 0.05378397243718306, 0.043757897491256394, 0.045885120828946434, 0.03479916291932265, 0.03491066209971905, 0.03193995729088783, 0.02336003351956606, 0.02382470667362213, 0.02426537623008092, 0.02202051381270091, 0.01925339177250862, 0.017039965838193893, 0.02054562217866381, 0.013963413269569477, 0.017147580316911142, 0.014062242271999517, 0.013191633857786655, 0.01344482290248076, 0.0136890122666955, 0.010805041994899511, 0.013502251201619705], [0.5111277401447296, 0.10873769285778205, 0.023598760715685785, 0.010350338726614913, 0.0063173761979366345, 0.005558983248192817, 0.004396405885927379, 0.003239935690847536, 0.0035410214331932366, 0.003235540585592389, 0.003219309631579866, 0.0032049012176382044, 0.0030275005653190115, 0.0029572119237855077, 0.0029258418168562153, 0.0029449440577688315, 0.0025206738306830325, 0.0031789036196035645, 0.0032402490614913404, 0.0031499735972223184, 0.002754001955812176, 0.0027789527472729483, 0.002681931024805332, 0.002488606066132585, 0.0030785337051687143, 0.002551752181413273, 0.0027110726126314453, 0.0024759925242202976, 0.0036073835605445006, 0.003118496931468447]]]\n","[[1.3715818325678508, 0.4126656502485275, 0.19604775309562683, 0.1502742494146029, 0.11269257714351018, 0.10182273636261623, 0.08767394845684369, 0.07633837064107259, 0.06277272974451382, 0.05415709378818671, 0.04732719622552395, 0.04689803719520569, 0.03980153395483891, 0.03825698482493559, 0.03477091249078512, 0.03544411528855562, 0.027951185281078022, 0.029392659043272335, 0.027473734691739082, 0.025203295052051544, 0.020442602845529716, 0.022353791010876495, 0.028795068617910147, 0.023675909265875816, 0.020731040742248297, 0.022845350361118715, 0.018259093320618074, 0.019046569398293894, 0.015540611619750658, 0.017394320107996464], [0.6207434013485909, 0.2949373150865237, 0.14040536992251873, 0.0850533548121651, 0.06694333666625123, 0.05988390324637294, 0.05710918002296239, 0.05077174573671073, 0.042681279902656875, 0.04626736665765444, 0.0379576178578039, 0.04560780245810747, 0.039735860734557114, 0.03546296902156124, 0.040996137696007885, 0.038306495368791126, 0.04097226223287483, 0.03583694313419983, 0.03951062851895889, 0.0382942419188718, 0.0373565008242925, 0.04190335131715983, 0.04196848017939677, 0.04337817147218933, 0.04868813770978401, 0.04171939341661831, 0.047763546424297, 0.037380078729862966, 0.048431778559461236, 0.041226377012208104]]]\n","[[1.5287146667639415, 0.5059141566356024, 0.22676779081424078, 0.14395602295796076, 0.11816719671090443, 0.09394338726997375, 0.08413836359977722, 0.0681781539072593, 0.050339466581741966, 0.05474787453810374, 0.044431619346141815, 0.04131368827074766, 0.037509274979432426, 0.03236765072991451, 0.02876287108908097, 0.029336289192239445, 0.026741852362950642, 0.021680186813076336, 0.023210087791085243, 0.01708966555694739, 0.01743607424820463, 0.01772743525604407, 0.01669716291750471, 0.014263763558119535, 0.016424392815679312, 0.014474671334028244, 0.013314865529537201, 0.009285792087515196, 0.008411515659342209, 0.008606578378627697], [0.5039249137043953, 0.11614641112585862, 0.022392051216835778, 0.007191482970180611, 0.0041793266621728735, 0.0031197581362600126, 0.002199970961858829, 0.001666531238394479, 0.001701573278599729, 0.0013119124729807179, 0.001690031048686554, 0.0013008846435695887, 0.0008041171822696924, 0.0012739126104861498, 0.0008072534886499246, 0.000760202839349707, 0.0007899290843245884, 0.0007419381678725282, 0.000546755773636202, 0.0010478370628940563, 0.0008122862976354858, 0.0006501272243137161, 0.0006903721950948238, 0.0005863619347413381, 0.0005892555927857757, 0.000624214745281885, 0.0013057741064888735, 0.0005526560125872493, 0.0008998143409068385, 0.0004890330213432511]]]\n","[[1.3733632663885753, 0.4364018887281418, 0.19521871705849966, 0.13672695929805437, 0.11016702155272166, 0.0859388696650664, 0.08272070437669754, 0.06819870478163163, 0.0588926567385594, 0.04863864462822676, 0.045091321070988975, 0.03464728438605865, 0.03156925768901905, 0.02921194490045309, 0.028147117234766483, 0.025327206899722416, 0.01896890268350641, 0.02302939910441637, 0.01907620718702674, 0.013361365223924318, 0.01591187467177709, 0.015740244028468926, 0.01603513505930702, 0.011937295086681843, 0.012090435717254877, 0.015273674856871367, 0.012727931917955479, 0.012233519771446785, 0.008242597182591757, 0.01038894196972251], [0.6383766407767931, 0.29131894558668137, 0.1414302857592702, 0.0855936702961723, 0.06829800099755327, 0.06079887102047602, 0.054165554562738784, 0.03903216205071658, 0.04091416799928993, 0.03695652517490089, 0.030002281496611733, 0.032868688576854765, 0.027429708357279498, 0.02890292787924409, 0.03235516103450209, 0.021896354349640507, 0.025876814033836126, 0.023675287569252152, 0.02017653714089344, 0.020546693141416956, 0.020972142888543505, 0.017276829215309892, 0.01739901878560583, 0.019254909983525675, 0.016571180739750464, 0.01436590818533053, 0.012428534178373715, 0.01645120211954539, 0.01580084813758731, 0.01334290049271658]]]\n","[[1.5714350541432698, 0.6997162401676178, 0.3717923363049825, 0.24219808727502823, 0.1823792283733686, 0.14939881364504495, 0.13150677954157194, 0.12320637206236522, 0.11523287867506345, 0.11251868059237798, 0.10403565317392349, 0.09999989842375119, 0.10607921207944553, 0.10010204091668129, 0.10450994471708934, 0.10387439653277397, 0.1088252527018388, 0.10254559045036633, 0.10464947422345479, 0.10247482483585675, 0.09418409193555514, 0.09224714835484822, 0.0818328782916069, 0.07460808629790942, 0.06756602724393208, 0.06530128667751949, 0.06473900626103084, 0.06375051103532314, 0.06281842477619648, 0.05896861727039019], [0.5641667644182841, 0.23897724598646164, 0.09164462331682444, 0.05250100946674744, 0.041397988641013704, 0.039306208956986666, 0.04286641037712494, 0.05057845932121078, 0.05736454560731848, 0.06924131885170937, 0.07852008814613025, 0.0964756403118372, 0.11997735500335693, 0.15061409833530584, 0.18634405018140873, 0.218185152237614, 0.23730905136714378, 0.24912240418295065, 0.26716380783667165, 0.27722237693766755, 0.2783035483832161, 0.27906495224063593, 0.27452346907618147, 0.268374945425118, 0.2595539168299486, 0.25556150656969595, 0.25519986380822957, 0.25414517873044434, 0.2536024458628769, 0.25303481208781403]]]\n","[[1.7877225081125896, 0.7620594203472137, 0.3868165264527003, 0.24441873530546823, 0.19237747540076575, 0.1578033541639646, 0.142807903389136, 0.14880918835600218, 0.12609928846359253, 0.12495869273940723, 0.10665011405944824, 0.1086191398402055, 0.10918235530455907, 0.0977395089964072, 0.1025505264600118, 0.09089517593383789, 0.08835255975524585, 0.08337567994991939, 0.08253009989857674, 0.07095872238278389, 0.07641545496881008, 0.08108128234744072, 0.07974057209988435, 0.07746951157848041, 0.07285181246697903, 0.0828239011267821, 0.07919708763559659, 0.08466010789076488, 0.07309434687097867, 0.07325778156518936], [0.6690378387769064, 0.4434545288483302, 0.27031829953193665, 0.18039120423297086, 0.13639069069176912, 0.12344594237705071, 0.11148634118338425, 0.11839464958757162, 0.10594135010614991, 0.11280291558553775, 0.10637388626734416, 0.10849984797338645, 0.1096920430039366, 0.1061947828469177, 0.10407696850597858, 0.09702780252943437, 0.09905256936326623, 0.102167004874597, 0.10796663289268811, 0.12486628443002701, 0.1310020610690117, 0.12895197918017706, 0.1271263932188352, 0.12630934733897448, 0.12045254123707612, 0.12570484603444734, 0.11879709394027789, 0.12648357544094324, 0.12782201170921326, 0.12493713479489088]]]\n","[[1.639176845550537, 0.7059988975524902, 0.3556700100501378, 0.23021569103002548, 0.1705125297109286, 0.14868703608711561, 0.1204643485446771, 0.10916024819016457, 0.10037074734767278, 0.09609103078643481, 0.0883048710723718, 0.07692422904074192, 0.06997485583027203, 0.06700512642661731, 0.06753131002187729, 0.05610952898859978, 0.0584607378890117, 0.051676470786333084, 0.05021307369073232, 0.04627392999827862, 0.04564586250732342, 0.03571821687122186, 0.03859529706339041, 0.037565238773822784, 0.038488086933890976, 0.03404949481288592, 0.03728890170653661, 0.03420619107782841, 0.03293742053210735, 0.03573046252131462], [0.6152889877557755, 0.2535911152760188, 0.084485639197131, 0.036449397603670754, 0.022840907331556082, 0.01644968567416072, 0.013360220318039259, 0.012069118597234288, 0.011326408945024014, 0.011054061857673029, 0.010318931075744331, 0.010200966460009417, 0.009721498742389182, 0.009222728995761523, 0.009091572991261879, 0.009882087780473134, 0.008601098554208875, 0.008661175767580668, 0.00819126070321848, 0.008708444850829741, 0.0076504585255558295, 0.008492088837859532, 0.009249814708406726, 0.008027918171137571, 0.007919662709658345, 0.007458684519709398, 0.008758647697201619, 0.007901987681786219, 0.008019444571497539, 0.008140969051358601]]]\n","[[1.765800138314565, 0.745559553305308, 0.37158574163913727, 0.2342768336335818, 0.17554118732611337, 0.15581166744232178, 0.13089431573947272, 0.12426973258455594, 0.1134108416736126, 0.09832936276992162, 0.0934465837975343, 0.09028150265415509, 0.0779607171813647, 0.07715869260330994, 0.06895718599359195, 0.06642193160951138, 0.06237078892687956, 0.05906122736632824, 0.04905850191911062, 0.052667571852604546, 0.05548541465153297, 0.04700211063027382, 0.050128389770785965, 0.04185477395852407, 0.04255927074700594, 0.040292492136359215, 0.03895037900656462, 0.04207605558137099, 0.041463339080413185, 0.03989027999341488], [0.648735428849856, 0.44440971314907074, 0.2632005736231804, 0.1710951334486405, 0.1258532122398416, 0.09452017831305663, 0.0820824831413726, 0.0767174072874089, 0.06874333585922916, 0.06481693351330857, 0.06242936057969928, 0.05539677004950742, 0.052354880375787616, 0.05446600650126735, 0.05593573395162821, 0.049722027809669576, 0.04844820227784415, 0.04859875010636946, 0.044800992434223495, 0.04351023840717971, 0.04678850085474551, 0.04132976088051995, 0.03867999250845363, 0.040818490010375776, 0.04141279841617992, 0.04029781099719306, 0.04163264948874712, 0.04042023878234128, 0.03711054380983114, 0.03702198921625192]]]\n","[[1.5735624035199482, 0.7206015835205714, 0.375103622674942, 0.22100097934405008, 0.1775227834781011, 0.13606998696923256, 0.1293572075664997, 0.10740897183616956, 0.10307130962610245, 0.09648369873563449, 0.08832389550904433, 0.08174335087339084, 0.07135421968996525, 0.06711418305834134, 0.06292182517548402, 0.05804471671581268, 0.056601486479242645, 0.05098134030898412, 0.04955767281353474, 0.04244431480765343, 0.04770637924472491, 0.039897704807420574, 0.03387550637125969, 0.03873305954039097, 0.03321497763196627, 0.033131944946944714, 0.030828953410188358, 0.03836069100846847, 0.03335123974829912, 0.03355542880793413], [0.5801796168088913, 0.23552383047839007, 0.07347582094371319, 0.02892362520409127, 0.014474326861090958, 0.00943776243366301, 0.007388460333459079, 0.00591275355933855, 0.005444232859493543, 0.004512528384414812, 0.004419758450239897, 0.003967306305033465, 0.004306459391955286, 0.0034179987657504776, 0.0034920029575005174, 0.003312974236905575, 0.0026325885167655847, 0.0029763540951535106, 0.0024908165214583278, 0.002664821377644936, 0.0031271449794682362, 0.002620634564664215, 0.002303580268441389, 0.0026034963860486946, 0.0018897487413293372, 0.0016791712550912052, 0.0019627907507432005, 0.002012873320685079, 0.0019822769487897554, 0.002216082706581801]]]\n","[[1.7532773812611897, 0.787379115819931, 0.3940221319595973, 0.2429311399658521, 0.18798883259296417, 0.15836051354805628, 0.13060441240668297, 0.1153663843870163, 0.09823847065369289, 0.09752245619893074, 0.09005654106537501, 0.08531289796034495, 0.07732825353741646, 0.07266203065713246, 0.06916989386081696, 0.061941115806500115, 0.059455931186676025, 0.0561181070903937, 0.0491279112175107, 0.04452878454079231, 0.044244990373651184, 0.044153122852245964, 0.036625588002304234, 0.03819496463984251, 0.033735975002249084, 0.037177132442593575, 0.03468804185589155, 0.03517966344952583, 0.030565586561957996, 0.03249771427363157], [0.6526357879241308, 0.4257845878601074, 0.24598094945152602, 0.1541732493788004, 0.10828021572281916, 0.08419032006834944, 0.07283616904169321, 0.06075352911526958, 0.058549682687347136, 0.04954766505397856, 0.05000017563967655, 0.04341162147466093, 0.04459963692352176, 0.040960800931012877, 0.03881320570750783, 0.03943407050489137, 0.036395529246268175, 0.03236468675701568, 0.03693274798570201, 0.03551898281633233, 0.03405492473393679, 0.033514356977927186, 0.03026043262798339, 0.026619801453004282, 0.030184589297277853, 0.028365715911301475, 0.02815705423320954, 0.02710094137970979, 0.026847851928323507, 0.026361330839184422]]]\n","[[1.6696229378382366, 0.7489975939194361, 0.3790745139122009, 0.24031076083580652, 0.1828172653913498, 0.15642874936262766, 0.14343432461222014, 0.13115142161647478, 0.12819885462522507, 0.12212231010198593, 0.12891031180818877, 0.13124391809105873, 0.12191716333230336, 0.11474244172374408, 0.11436937749385834, 0.11732971792419751, 0.10839331522583961, 0.10245756184061368, 0.08472468207279842, 0.07837704879542191, 0.07795953005552292, 0.07292733651896317, 0.068418779099981, 0.06007577789326509, 0.05594313703477383, 0.052924041325847306, 0.05401496837536494, 0.051113575076063476, 0.04924863763153553, 0.0443412596359849], [0.5879935945073763, 0.2544400207698345, 0.10045058156053226, 0.06101434755449494, 0.05421607242897153, 0.06297215648616354, 0.0850982026507457, 0.1121054779117306, 0.1398122701793909, 0.16348807358493408, 0.1909111418450872, 0.21156434000780186, 0.23083181555072466, 0.24640309189756712, 0.26422131558259326, 0.2767354402070244, 0.278644609109809, 0.27822336833924055, 0.27285213077751297, 0.26780241122469306, 0.26057621898750466, 0.2550884831386308, 0.24898987150906274, 0.2440366652638962, 0.2379570670115451, 0.23234433106457195, 0.22610066736039394, 0.2200827007375968, 0.21471198005989814, 0.20882379116180042]]]\n","[[1.6839361786842346, 0.7492761611938477, 0.3670676648616791, 0.23944895466168722, 0.1864959473411242, 0.15450569490591684, 0.13913929089903831, 0.12360762059688568, 0.11709656566381454, 0.10882000625133514, 0.10331513111790021, 0.09591296687722206, 0.09202005590001743, 0.09556171918908755, 0.0837878944973151, 0.09007015824317932, 0.08536803349852562, 0.08683371543884277, 0.0824790671467781, 0.07726225008567174, 0.08373987426360448, 0.08490925841033459, 0.07139584794640541, 0.07612274959683418, 0.0683016541103522, 0.06203494034707546, 0.06118314526975155, 0.056901389732956886, 0.06473149669667085, 0.05688163017233213], [0.6564711878697077, 0.44356369227170944, 0.2689419314265251, 0.17459026103218397, 0.13304727027813593, 0.11247105834384759, 0.10527706611901522, 0.09963080861295263, 0.09728368495901425, 0.09282024918744962, 0.09058799967169762, 0.09145617733399074, 0.09629212164630492, 0.09250999490420024, 0.10098441503942013, 0.10588701612626512, 0.1030831824367245, 0.11296251385162275, 0.1104731266386807, 0.10966542611519496, 0.10666347714141011, 0.10641529535253842, 0.09849034529179335, 0.09624866892894109, 0.09405695864309867, 0.10342243841538827, 0.1046047971273462, 0.1133301433486243, 0.11313481566806634, 0.11544188763946295]]]\n","[[1.9344840447107952, 0.8056804239749908, 0.4140031884113948, 0.25978461652994156, 0.19636179755131403, 0.15942487865686417, 0.13657932976881662, 0.11996962875127792, 0.10711348429322243, 0.10035133113463719, 0.09371644134322803, 0.08392706637581189, 0.08069358766078949, 0.06880910818775494, 0.07418741658329964, 0.06826948436597984, 0.06400238846739133, 0.0608830563724041, 0.05803120384613673, 0.05617560756703218, 0.05531203746795654, 0.04727303826560577, 0.05378595242897669, 0.04449026845395565, 0.04525826002160708, 0.04575676967700323, 0.04244682161758343, 0.05245992665489515, 0.04582973507543405, 0.041692920650045075], [0.5825300514698029, 0.25060782209038734, 0.09214886898795764, 0.04561591520905495, 0.028489546074221533, 0.022311575322722394, 0.018798539725442726, 0.01730738712164263, 0.015828241671745975, 0.015381024063875278, 0.014618706152153512, 0.015371890234140059, 0.015983241765449446, 0.017461020421857636, 0.017149631050415337, 0.018437031966944534, 0.020996416841323178, 0.022561851151597995, 0.02553346656107654, 0.025582848854052525, 0.028763190377503633, 0.029824818794926006, 0.03390821861103177, 0.03809324376440296, 0.04224704920003811, 0.048442851984873414, 0.05417831133430203, 0.062034444184973836, 0.06905025926729043, 0.08446543865526716]]]\n","[[1.6715603669484456, 0.7236416439215342, 0.3717600554227829, 0.2387803221742312, 0.18018077313899994, 0.1489331734677156, 0.13484384988745055, 0.11858200530211131, 0.11190984770655632, 0.09422009934981664, 0.09704706072807312, 0.08477565459907055, 0.08327051252126694, 0.07395054151614507, 0.0667951920380195, 0.06424675819774468, 0.05987570062279701, 0.05894790465633074, 0.056092812990148865, 0.05073607340455055, 0.04958971403539181, 0.04378123643497626, 0.04211128999789556, 0.04513023172815641, 0.043995714746415615, 0.03944933072974285, 0.037365663486222424, 0.03413071638594071, 0.03339457232505083, 0.02954135959347089], [0.658639115591844, 0.45117949694395065, 0.27808968474467594, 0.18234657868742943, 0.13141294848173857, 0.10583735847224791, 0.0985112984975179, 0.0785537810685734, 0.07674441595251362, 0.07013591720412175, 0.06441337863604228, 0.06457336712628603, 0.06119411733622352, 0.054638458493476115, 0.055881807037318744, 0.05446488790524503, 0.05091825942508876, 0.053699588985182345, 0.057362032821401954, 0.05119254544842988, 0.050644980277866125, 0.05082389300999542, 0.05144431864997993, 0.050815694383345544, 0.04684403323335573, 0.04489392965721587, 0.055323357422215245, 0.04775924130808562, 0.04495885552993665, 0.04726688672478]]]\n","[[1.6552544832229614, 0.7270684639612833, 0.35969968636830646, 0.21568844964106879, 0.16627739618221918, 0.14217007284363112, 0.12353351215521495, 0.10741515705982844, 0.09977579489350319, 0.08649117002884547, 0.08192561380565166, 0.08308593680461247, 0.07463526353240013, 0.0638744222621123, 0.06532087177038193, 0.0511181503534317, 0.0502349982659022, 0.05427183707555135, 0.04721010600527128, 0.04959853831678629, 0.03960332181304693, 0.04196216414372126, 0.04234511063744625, 0.029635298376282055, 0.03329815715551376, 0.029867394516865414, 0.03737322116891543, 0.02537734992802143, 0.026316442526876926, 0.02645872828240196], [0.5934287259976069, 0.23714960118134817, 0.07222056140502293, 0.02653731615282595, 0.01349413460896661, 0.008606054354459047, 0.006478476493308942, 0.005412002908997238, 0.005173447076231241, 0.004083928225251536, 0.0047917244567846256, 0.004713693730688344, 0.004122509136019896, 0.003870467907593896, 0.0036744651151821017, 0.0033312186521167555, 0.003209787052279959, 0.002387337755256643, 0.00277299825878193, 0.0025202240018794933, 0.0026015533173146346, 0.0024166040626975396, 0.0019466598751023412, 0.0018154771338837843, 0.002000696452644964, 0.001995149767026305, 0.0025040798160868385, 0.002044380894706895, 0.0014975576971968014, 0.0018121923591631155]]]\n","[[1.8554776708285015, 0.7474808295567831, 0.3791520098845164, 0.24031476179758707, 0.17490016917387644, 0.14215589314699173, 0.12757314120729765, 0.10678673287232716, 0.10612041875720024, 0.09159079318245252, 0.07837031657497089, 0.07354862491289775, 0.0705026922126611, 0.06463017128407955, 0.062098163490494095, 0.05851431998113791, 0.05141019138197104, 0.055564618979891144, 0.050134168937802315, 0.04515678125123183, 0.047378877798716225, 0.03787985909730196, 0.03483369201421738, 0.04290148740013441, 0.039454746370514236, 0.0314119349544247, 0.031555124558508396, 0.03135525155812502, 0.027846114089091618, 0.028089225913087528], [0.6504516104857127, 0.43485945959885913, 0.25727998092770576, 0.16054243532319865, 0.1103526105483373, 0.08782420260831714, 0.0759238450943182, 0.06583095372964938, 0.057825940350691475, 0.050827959164356194, 0.048949706251733005, 0.049809516950820885, 0.04227538779377937, 0.043283585323176034, 0.04230925584367166, 0.03924807737348601, 0.0398409494276469, 0.03858259413391352, 0.03851806100768348, 0.033717318127552666, 0.03600661831054216, 0.03307608177419752, 0.02762281462006892, 0.02713978294438372, 0.028349585947580636, 0.030717522487975657, 0.025817217033666868, 0.024605071870610118, 0.02529055590275675, 0.026199590919228893]]]\n","[[2.093733469645182, 1.570462425549825, 1.1196664075056713, 0.8603172500928243, 0.6938590208689371, 0.5782399574915568, 0.498462309439977, 0.43807310859362286, 0.38364173968633014, 0.34892621139685315, 0.3129231631755829, 0.3040875494480133, 0.27811171611150104, 0.25703394909699756, 0.24038910120725632, 0.23009517043828964, 0.2222402592500051, 0.2149063472946485, 0.21156363437573114, 0.20333404590686163, 0.18873954564332962, 0.18949797252813974, 0.18595517426729202, 0.17822324484586716, 0.177045909066995, 0.1748239149649938, 0.17242148021856943, 0.1740446351468563, 0.17474102973937988, 0.17592659344275793], [0.6502366165320078, 0.535057875017325, 0.4091363027691841, 0.30198535571495694, 0.232498316715161, 0.18443483114242554, 0.15395773885150751, 0.13521166518330574, 0.12082409796615441, 0.11102033344407876, 0.10248607645432155, 0.09808629440764587, 0.09355701071520646, 0.09207815770059824, 0.09016297881801923, 0.0897451542938749, 0.08880749282737573, 0.09123041480779648, 0.08851930033415556, 0.08955853556593259, 0.0930855975796779, 0.09323251899331808, 0.09721135006596644, 0.09729381122936805, 0.10000162788977225, 0.10004376775274675, 0.10166207576791446, 0.1002241459985574, 0.1004195815573136, 0.10420510328064363]]]\n","[[2.011741578578949, 1.5046585202217102, 1.0780500769615173, 0.8206293880939484, 0.658433218797048, 0.5409820775190989, 0.46247807145118713, 0.41481676201025647, 0.35044590135415393, 0.33186913033326465, 0.29880038400491077, 0.2823782240351041, 0.2765352974335353, 0.2535992314418157, 0.23467284192641577, 0.22682434072097143, 0.21530998994906744, 0.2109019805987676, 0.20235860844453177, 0.19594988226890564, 0.19568902254104614, 0.1784002035856247, 0.18042604625225067, 0.17697961131731668, 0.17317227025826773, 0.165226419766744, 0.16432833671569824, 0.162783395498991, 0.1609615460038185, 0.1679264431198438], [0.6972506592671076, 0.6390222782890002, 0.5614594320456187, 0.4792582591374715, 0.4193059826890628, 0.36841943114995956, 0.32351455837488174, 0.29343755170702934, 0.25934237986803055, 0.24317168444395065, 0.22288359329104424, 0.21193419893582663, 0.19541552662849426, 0.18561565689742565, 0.17816039298971495, 0.16809635671476522, 0.1621276910106341, 0.15681318752467632, 0.1517534169058005, 0.14900228939950466, 0.14797719257573286, 0.14081338265289864, 0.13698736050476631, 0.13637001315752664, 0.13518866586188474, 0.12904071000715098, 0.12994931700328985, 0.12853288960953554, 0.12929002630213896, 0.1308731564010183]]]\n","[[2.0274137457211814, 1.5346064964930217, 1.087752530972163, 0.8380902806917826, 0.6580725411574045, 0.5529475510120392, 0.47386106352011365, 0.4050145447254181, 0.36586426695187885, 0.3373194833596547, 0.3046708405017853, 0.27831634879112244, 0.2581893503665924, 0.243617149690787, 0.23451181252797446, 0.21569268157084784, 0.2185462017854055, 0.20509719600280127, 0.19011713564395905, 0.1939974973599116, 0.17180059850215912, 0.17692979921897253, 0.15891228492061296, 0.16481686880191168, 0.15569577366113663, 0.15548215061426163, 0.15864326804876328, 0.14769458522399267, 0.14998984212676683, 0.14802518983681998], [0.6975926260153452, 0.5692248766620954, 0.4267623896400134, 0.3120831176638603, 0.23019854724407196, 0.18007957811156908, 0.14291537180542946, 0.11856896926959355, 0.10237626172602177, 0.08938742677370708, 0.08170713980992635, 0.07363278542955716, 0.0675248463327686, 0.0612531341612339, 0.0591969753925999, 0.05526626016944647, 0.05160426131139199, 0.04888817792137464, 0.04742904752492905, 0.04510886361822486, 0.04500540749480327, 0.04194512482111653, 0.038868956423054137, 0.0391257352506121, 0.037788286029050745, 0.03682435614367326, 0.035498710038761296, 0.03593859619771441, 0.03809872688725591, 0.03607199046139916]]]\n","[[1.7834110061327617, 1.363802433013916, 0.9807410538196564, 0.7644849022229513, 0.6237389842669169, 0.5054123451312383, 0.44415444135665894, 0.37446436782677966, 0.3295021702845891, 0.29839936395486194, 0.2797243098417918, 0.2580665896336238, 0.2459145039319992, 0.23428498208522797, 0.22736441095670065, 0.21239298582077026, 0.20488018542528152, 0.18898975352446237, 0.17820991824070612, 0.17670579254627228, 0.18033750603596369, 0.1678552304704984, 0.16096924742062887, 0.16172047952810922, 0.1560461719830831, 0.15241180111964545, 0.14738096420963606, 0.1511057193080584, 0.14450636381904283, 0.14928779751062393], [0.7108181367317835, 0.6478290806214014, 0.5633244613806406, 0.488829863568147, 0.4154042551914851, 0.3592434972524643, 0.31953539699316025, 0.27734477693835896, 0.25363973528146744, 0.23125253369410834, 0.21124103789528212, 0.1986325935771068, 0.18255527379612127, 0.1652038705845674, 0.1588777837653955, 0.14941426739096642, 0.14140672733386359, 0.13795546318093935, 0.13056952444215617, 0.12673297400275865, 0.12027848387757938, 0.11670387350022793, 0.10908397225042184, 0.10684557817876339, 0.10744890632728736, 0.1026086863130331, 0.10007455448309581, 0.10351571875313918, 0.10122885803381602, 0.10065014380961657]]]\n","[[2.0080788334210715, 1.5276095072428386, 1.0700792074203491, 0.8331456383069357, 0.6673048138618469, 0.542522519826889, 0.4551785985628764, 0.3751520961523056, 0.34810222188631695, 0.30416453381379444, 0.2865154594182968, 0.265008124212424, 0.2370041012763977, 0.23236571997404099, 0.21826687206824621, 0.20141889651616415, 0.1944257616996765, 0.17707050840059915, 0.174894779920578, 0.17581248035033545, 0.1646207496523857, 0.15302089850107828, 0.15258149926861128, 0.15148277829090753, 0.14775047327081361, 0.14280936370293298, 0.14958554009596506, 0.13907071327169737, 0.15038993209600449, 0.15167640646298727], [0.680850679675738, 0.5577255537112554, 0.4209107880791028, 0.30299052596092224, 0.22515146310130754, 0.16524832447369894, 0.12759599337975183, 0.10186196304857731, 0.08660172050197919, 0.0709372761969765, 0.06092367352296909, 0.05352496479948362, 0.04689884465187788, 0.04280191954846183, 0.03941510198637843, 0.0358171119975547, 0.03401981573551893, 0.030510013767828543, 0.028941800817847252, 0.027108695823699236, 0.0246320441365242, 0.02290657131622235, 0.022116267898430426, 0.02095433014134566, 0.019614991343890626, 0.019811429704229038, 0.018338915503894288, 0.018597200823326904, 0.01839858681584398, 0.01895762700587511]]]\n","[[2.0989625056584678, 1.6016812920570374, 1.1137581964333851, 0.8509538074334463, 0.6737987200419108, 0.5535061359405518, 0.46286699672540027, 0.402328684926033, 0.35871800283590954, 0.30756907165050507, 0.28640081981817883, 0.2683703849713008, 0.24434874206781387, 0.23084631810585657, 0.22043412427107492, 0.21312670161326727, 0.19753673672676086, 0.19068224479754767, 0.17930931597948074, 0.1733056977391243, 0.16739039619763693, 0.15668393050630888, 0.1580479914943377, 0.1462340479095777, 0.15002441654602686, 0.1441563256084919, 0.14524058749278387, 0.1400535081823667, 0.1464324581126372, 0.14367463936408362], [0.7288754284381866, 0.6464221626520157, 0.5631294449170431, 0.4791796455780665, 0.4081682165463765, 0.3485733283062776, 0.3078545741736889, 0.26787833000222844, 0.23772332817316055, 0.21509913665552935, 0.1948319977770249, 0.17568628986676535, 0.16640680159131685, 0.14942530604700247, 0.14155444875359535, 0.13422176210830608, 0.12497286777943373, 0.11788536173601945, 0.11469304002821445, 0.10825331105540197, 0.10099895081172387, 0.10201476498817404, 0.09417052768791716, 0.09098280097047488, 0.09008681991448005, 0.08683543217678864, 0.08391867764294147, 0.08838953621064623, 0.08462240888426702, 0.08785089881469806]]]\n","[[2.2528175910313926, 1.7016470630963643, 1.1890620390574138, 0.8854022920131683, 0.7149385809898376, 0.6016948223114014, 0.5118244041999181, 0.4321081538995107, 0.39141522844632465, 0.35265496869881946, 0.31073983510335285, 0.2978446880976359, 0.27609457820653915, 0.2596883699297905, 0.24417191992203394, 0.2316104769706726, 0.2163665940364202, 0.22061949719985327, 0.2039644569158554, 0.20169570297002792, 0.1965179666876793, 0.1890069618821144, 0.1848356897632281, 0.1725970854361852, 0.17303204288085303, 0.17190178483724594, 0.1724321444829305, 0.1628188192844391, 0.16141064713398615, 0.15760929137468338], [0.6872637470563253, 0.5599989319841067, 0.4180785045027733, 0.306211909900109, 0.22977331032355627, 0.18023844808340073, 0.15073149154583612, 0.13222466657559076, 0.11516617486874263, 0.10620271600782871, 0.0984602967898051, 0.0930873869607846, 0.0873839712391297, 0.08601682198544343, 0.08371592996021111, 0.08318603131920099, 0.08276892453432083, 0.08202324621379375, 0.08155021040389936, 0.082685688820978, 0.08334712715198596, 0.08559968632956345, 0.08757293379555146, 0.08743749155352513, 0.08865623082965612, 0.09284342421839635, 0.09162492770701647, 0.0928667892391483, 0.0953842584664623, 0.09474502659092347]]]\n","[[1.826530655225118, 1.397868573665619, 1.0175273021062214, 0.7921478648980459, 0.6161737243334452, 0.517559289932251, 0.43913954496383667, 0.37049493193626404, 0.3343048691749573, 0.3091036876042684, 0.29415465394655865, 0.2615126396218936, 0.25423667083183926, 0.2387966811656952, 0.2214131678144137, 0.21008371810118356, 0.1954784244298935, 0.1966816633939743, 0.19295828541119894, 0.17662828912337622, 0.17919907967249551, 0.1684670920173327, 0.16443912436564764, 0.15896846229831377, 0.16304548333088556, 0.15118795881668726, 0.1428738310933113, 0.14717702319224676, 0.14133651678760847, 0.1430826298892498], [0.709709698955218, 0.644705188771089, 0.5555214087168375, 0.4843677605191867, 0.41288987795511883, 0.35759777079025906, 0.3142547855774562, 0.2811155418554942, 0.2535715053478877, 0.2312824328740438, 0.2116328583409389, 0.19887535522381464, 0.18523293423155943, 0.17441882627705732, 0.16483954836924872, 0.15958760182062784, 0.14953972399234772, 0.14271440481146178, 0.14144071595122418, 0.1351490281522274, 0.13287634495645761, 0.1273629575346907, 0.12186550224820773, 0.118001537087063, 0.11427273290852706, 0.11284332629293203, 0.11006898308793704, 0.11002357490360737, 0.10698654254277547, 0.1038565852989753]]]\n","[[2.082730750242869, 1.5675281882286072, 1.1455855965614319, 0.8682788610458374, 0.6940894722938538, 0.5695210645596186, 0.47836655875047046, 0.4113381803035736, 0.35333148141702014, 0.3212439964214961, 0.2936839312314987, 0.2786227613687515, 0.25759289413690567, 0.24619906892379126, 0.2343181918064753, 0.2293678273757299, 0.2026069611310959, 0.19920923809210458, 0.19425462186336517, 0.1832060987750689, 0.18139908959468207, 0.1825709193944931, 0.16080687070886293, 0.16290725767612457, 0.1579757792254289, 0.1554814651608467, 0.1491509365538756, 0.14808091893792152, 0.13724696015318236, 0.13997906198104224], [0.6899821559588114, 0.5723050857583681, 0.4293475076556206, 0.31239616374174756, 0.2308000512421131, 0.17989002913236618, 0.1432650089263916, 0.11708937088648479, 0.09985443328817685, 0.08758241807421048, 0.07867051661014557, 0.0704853671292464, 0.06471200070033471, 0.05879898897061745, 0.055365622974932194, 0.051957133846978344, 0.04852294735610485, 0.045442488665382065, 0.04384533719470104, 0.04155189481874307, 0.039431963892032705, 0.03939835820347071, 0.03759357240051031, 0.03489601177473863, 0.03464103722944856, 0.03307425448050102, 0.032609200881173216, 0.03152360456685225, 0.03090774454176426, 0.030091052409261465]]]\n","[[1.9644628365834553, 1.4644563396771748, 1.0336991846561432, 0.78169184923172, 0.6448109249273936, 0.5291532079378763, 0.4554348737001419, 0.39636720220247906, 0.36286187171936035, 0.33979933460553485, 0.3095998167991638, 0.2888547231753667, 0.26850639780362445, 0.25106181204319, 0.23088829716046652, 0.2240989233056704, 0.2087776909271876, 0.20791141440471014, 0.2067083219687144, 0.19069387763738632, 0.18210037797689438, 0.1783802087108294, 0.17302912722031275, 0.1671363984545072, 0.16575427477558455, 0.1610481763879458, 0.1528840164343516, 0.15180563926696777, 0.1430839498837789, 0.1422817756732305], [0.7182434946298599, 0.653688058257103, 0.5721807926893234, 0.49258411178986233, 0.42075279355049133, 0.36105357110500336, 0.31819510211547214, 0.2770263192554315, 0.2492592173318068, 0.22186986977855364, 0.20743915252387524, 0.19078758048514524, 0.1779262392471234, 0.16564000522096953, 0.1519023316601912, 0.14764015873273215, 0.13462086052944264, 0.13422807616492113, 0.12611121156563362, 0.11951728413502376, 0.11415697727352381, 0.11045702577879031, 0.10606429073959589, 0.10595723614096642, 0.10238045019408067, 0.10243021650239825, 0.09686761815100908, 0.09289613040164113, 0.08989352562154333, 0.0904751056805253]]]\n","[[2.1150267918904624, 1.5678439339001973, 1.0846533179283142, 0.8319942553838094, 0.6470736165841421, 0.536959782242775, 0.44162050386269885, 0.39073558151721954, 0.3433760603268941, 0.29720813284317654, 0.2799985110759735, 0.26059067249298096, 0.24130752434333166, 0.2246080289284388, 0.21603099753459296, 0.2107541263103485, 0.19483539710442224, 0.18729636818170547, 0.17063885678847632, 0.17884804556767145, 0.16085319221019745, 0.1617156465848287, 0.15380366270740828, 0.15535819282134375, 0.14406380554040274, 0.13283012807369232, 0.1346255031724771, 0.13158579170703888, 0.13101087510585785, 0.12076161180933316], [0.6725628525018692, 0.5535367603103319, 0.4056559031208356, 0.29145796224474907, 0.20759962995847067, 0.15182639844715595, 0.1177224622418483, 0.09287838637828827, 0.07753953617066145, 0.06431364578505357, 0.05621536013980707, 0.049608832535644375, 0.04375261875490347, 0.03864331574489673, 0.0359303600465258, 0.032045589139064155, 0.0293890784184138, 0.027140777247647446, 0.026500030886381865, 0.024248434385905664, 0.022553478367626667, 0.022158989993234474, 0.020568814283857744, 0.01907374430447817, 0.018558568165947992, 0.017388504076128204, 0.016367671235154074, 0.01621846326937278, 0.015872415155172348, 0.015046647284179926]]]\n","[[]]\n"],"name":"stdout"}]}]}